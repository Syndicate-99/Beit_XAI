{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "from PIL import Image\n",
    "import gc\n",
    "import warnings\n",
    "from tqdm import tqdm\n",
    "from transformers import BeitModel, BeitConfig, AutoImageProcessor\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score, precision_recall_curve, auc\n",
    "import timm\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F  # Ensure this is imported\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR, ReduceLROnPlateau\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set seeds for reproducibility\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed()\n",
    "\n",
    "# Enhanced configuration for 99% accuracy\n",
    "CONFIG = {\n",
    "    'data_dir': \"D:/Work/project work/archive_lungs/chest_xray/chest_xray\",\n",
    "    'batch_size': 8,\n",
    "    'eval_batch_size': 8,\n",
    "    'num_epochs': 50,\n",
    "    'learning_rate': 0.00002,  # Lower learning rate for more precise convergence\n",
    "    'image_size': 384,  # Higher resolution for more detailed features\n",
    "    'beit_model': \"microsoft/beit-large-patch16-384\",  # Larger, higher-capacity model\n",
    "    'use_mixed_precision': True,\n",
    "    'gradient_accumulation_steps': 2,\n",
    "    'weight_decay': 0.05,  # Higher weight decay to prevent overfitting\n",
    "    'class_weight_normal': 1.5,\n",
    "    'class_weight_pneumonia': 1.0,\n",
    "    'augmentation_strength': 'extreme',  # Very strong augmentations\n",
    "    'cutmix_prob': 0.5,  # CutMix augmentation probability\n",
    "    'mixup_prob': 0.5,  # MixUp augmentation probability\n",
    "    'mixup_alpha': 0.4,\n",
    "    'scheduler': 'cosine_with_warmup',\n",
    "    'warmup_epochs': 5,\n",
    "    'patience': 10,  # More patience for early stopping\n",
    "    'use_test_time_augmentation': True,  # Use TTA for better inference\n",
    "    'tta_transforms': 5,  # Number of test-time augmentations\n",
    "    'use_ensemble': True,  # Use model ensemble for best results\n",
    "    'ensemble_weights': [0.7, 0.3],  # Weights for ensemble models\n",
    "    'use_ema': True,  # Exponential moving average for model weights\n",
    "    'ema_decay': 0.999,\n",
    "    'clip_grad_norm': 1.0,\n",
    "    'use_cosine_annealing': True,\n",
    "    'final_div_factor': 1e4,  # Maximum lr reduction factor\n",
    "    'plateau_patience': 5,  # Patience for learning rate reduction\n",
    "    'plateau_factor': 0.5,  # Factor by which learning rate will be reduced\n",
    "    'min_lr': 1e-7,  # Minimum learning rate\n",
    "    'dropout_rate': 0.3,  # Dropout rate for regularization\n",
    "    'stochastic_depth_prob': 0.1,  # Stochastic depth probability\n",
    "    'label_smoothing': 0.1,  # Label smoothing factor\n",
    "    'use_advanced_normalization': True,  # Use more advanced normalization techniques\n",
    "    'use_timm': True,  # Use timm library for model\n",
    "    'timm_model': 'beit_large_patch16_384',  # Timm model name\n",
    "    'use_dynamic_loss_scaling': True,  # Dynamic loss scaling for mixed precision\n",
    "    'exponential_moving_average': True,  # Use EMA for model weights\n",
    "    'val_percent': 0.1,  # Percentage of data to use for validation\n",
    "    'balanced_sampler': True,  # Use balanced sampler for class balance\n",
    "    'lookahead_optimizer': True,  # Use lookahead optimizer\n",
    "    'lookahead_k': 5,  # Lookahead parameter k\n",
    "    'lookahead_alpha': 0.5,  # Lookahead parameter alpha\n",
    "    'swa_start': 30,  # When to start Stochastic Weight Averaging\n",
    "    'swa_freq': 5,  # Frequency of SWA model updates\n",
    "    'swa_lr': 0.001,  # SWA learning rate\n",
    "    'training_visualization': True,  # Visualize training progress\n",
    "    'target_accuracy': 0.99,  # Target accuracy\n",
    "    'precision_weight': 0.5,  # Weight for precision in custom loss\n",
    "    'recall_weight': 0.5,  # Weight for recall in custom loss\n",
    "    'use_weighted_ensemble': True,  # Use weighted ensemble\n",
    "    'pixel_normalization': True,  # Use pixel normalization\n",
    "    'use_multiscale_inputs': True,  # Use multi-scale inputs\n",
    "    'use_progressive_resizing': False,  # Use progressive resizing\n",
    "    'start_size': 384,  # Starting image size\n",
    "    'progressive_epochs': [10, 20, 30],  # Epochs to increase size\n",
    "    'progressive_sizes': [256, 320, 384],  # Progressive sizes\n",
    "    'use_multi_stage_training': True,  # Train model in multiple stages\n",
    "    'geometric_aug_prob': 0.75,  # Probability of geometric augmentations\n",
    "    'photometric_aug_prob': 0.75,  # Probability of photometric augmentations\n",
    "    'normalize_means': [0.5056, 0.5056, 0.5056],  # Custom normalization means\n",
    "    'normalize_stds': [0.252, 0.252, 0.252],  # Custom normalization stds\n",
    "    'advanced_post_processing': True,  # Apply advanced post-processing\n",
    "    'use_attention_pooling': True,  # Use attention pooling\n",
    "    'use_deep_supervision': True,  # Use deep supervision\n",
    "    'discriminative_lr': True,  # Use discriminative learning rates\n",
    "    'discriminative_lr_factor': 0.1,  # Factor between layer groups\n",
    "}\n",
    "\n",
    "# Device configuration\n",
    "torch.cuda.empty_cache()\n",
    "gc.collect()\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Memory optimization for CUDA\n",
    "if torch.cuda.is_available():\n",
    "    os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
    "    try:\n",
    "        torch.cuda.set_per_process_memory_fraction(0.9)\n",
    "    except:\n",
    "        pass\n",
    "    print(f\"Total GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")\n",
    "\n",
    "# Exponential Moving Average for model weights\n",
    "class ModelEMA:\n",
    "    def __init__(self, model, decay=0.9999):\n",
    "        self.model = model\n",
    "        self.decay = decay\n",
    "        self.shadow = {}\n",
    "        self.backup = {}\n",
    "        self.register()\n",
    "\n",
    "    def register(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                self.shadow[name] = param.data.clone()\n",
    "\n",
    "    def update(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                new_average = (1 - self.decay) * param.data + self.decay * self.shadow[name]\n",
    "                self.shadow[name] = new_average.clone()\n",
    "\n",
    "    def apply_shadow(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.shadow\n",
    "                self.backup[name] = param.data\n",
    "                param.data = self.shadow[name]\n",
    "\n",
    "    def restore(self):\n",
    "        for name, param in self.model.named_parameters():\n",
    "            if param.requires_grad:\n",
    "                assert name in self.backup\n",
    "                param.data = self.backup[name]\n",
    "        self.backup = {}\n",
    "\n",
    "# Lookahead optimizer wrapper\n",
    "class Lookahead(optim.Optimizer):\n",
    "    def __init__(self, optimizer, k=5, alpha=0.5):\n",
    "        self.optimizer = optimizer\n",
    "        self.k = k\n",
    "        self.alpha = alpha\n",
    "        self.param_groups = self.optimizer.param_groups\n",
    "        self.defaults = self.optimizer.defaults\n",
    "        self.state = defaultdict(dict)\n",
    "        self.fast_state = self.optimizer.state\n",
    "        for group in self.param_groups:\n",
    "            group['counter'] = 0\n",
    "\n",
    "    def update(self, group):\n",
    "        for fast in group['params']:\n",
    "            param_state = self.state[fast]\n",
    "            if 'slow_param' not in param_state:\n",
    "                param_state['slow_param'] = torch.zeros_like(fast.data)\n",
    "                param_state['slow_param'].copy_(fast.data)\n",
    "            slow = param_state['slow_param']\n",
    "            slow += self.alpha * (fast.data - slow)\n",
    "            fast.data.copy_(slow)\n",
    "\n",
    "    def update_lookahead(self):\n",
    "        for group in self.param_groups:\n",
    "            self.update(group)\n",
    "\n",
    "    def step(self, closure=None):\n",
    "        loss = self.optimizer.step(closure)\n",
    "        for group in self.param_groups:\n",
    "            group['counter'] += 1\n",
    "            if group['counter'] % self.k == 0:\n",
    "                self.update(group)\n",
    "        return loss\n",
    "\n",
    "    def state_dict(self):\n",
    "        fast_state_dict = self.optimizer.state_dict()\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in self.state.items()\n",
    "        }\n",
    "        fast_state = fast_state_dict['state']\n",
    "        param_groups = fast_state_dict['param_groups']\n",
    "        return {\n",
    "            'fast_state': fast_state,\n",
    "            'slow_state': slow_state,\n",
    "            'param_groups': param_groups,\n",
    "        }\n",
    "\n",
    "    def load_state_dict(self, state_dict):\n",
    "        fast_state_dict = {\n",
    "            'state': state_dict['fast_state'],\n",
    "            'param_groups': state_dict['param_groups'],\n",
    "        }\n",
    "        self.optimizer.load_state_dict(fast_state_dict)\n",
    "        slow_state = {\n",
    "            (id(k) if isinstance(k, torch.Tensor) else k): v\n",
    "            for k, v in state_dict['slow_state'].items()\n",
    "        }\n",
    "        self.state.update(slow_state)\n",
    "\n",
    "# Advanced transformations for extreme data augmentation\n",
    "class AdvancedTransforms:\n",
    "    @staticmethod\n",
    "    def get_transforms(mode='train', augmentation_strength='extreme', image_size=384):\n",
    "        if mode == 'train':\n",
    "            if augmentation_strength == 'extreme':\n",
    "                return transforms.Compose([\n",
    "                    transforms.Resize((image_size, image_size)),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomRotation(30),\n",
    "                    ], p=0.7),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomAffine(\n",
    "                            degrees=20, \n",
    "                            translate=(0.2, 0.2), \n",
    "                            scale=(0.8, 1.2),\n",
    "                            shear=20\n",
    "                        ),\n",
    "                    ], p=0.5),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.GaussianBlur(kernel_size=5, sigma=(0.1, 2.0)),\n",
    "                    ], p=0.3),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.ColorJitter(\n",
    "                            brightness=0.3,\n",
    "                            contrast=0.3,\n",
    "                            saturation=0.2,\n",
    "                            hue=0.1\n",
    "                        ),\n",
    "                    ], p=0.7),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomPerspective(distortion_scale=0.3, p=1.0),\n",
    "                    ], p=0.3),\n",
    "                    transforms.RandomVerticalFlip(p=0.3),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomAutocontrast(p=1.0),\n",
    "                    ], p=0.3),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomEqualize(p=1.0),\n",
    "                    ], p=0.3),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomAdjustSharpness(sharpness_factor=2.0, p=1.0),\n",
    "                    ], p=0.3),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=CONFIG['normalize_means'],\n",
    "                        std=CONFIG['normalize_stds']\n",
    "                    ),\n",
    "                    transforms.RandomApply([\n",
    "                        transforms.RandomErasing(\n",
    "                            p=1.0, \n",
    "                            scale=(0.02, 0.2), \n",
    "                            ratio=(0.3, 3.3), \n",
    "                            value=0\n",
    "                        ),\n",
    "                    ], p=0.4),\n",
    "                ])\n",
    "            elif augmentation_strength == 'strong':\n",
    "                return transforms.Compose([\n",
    "                    transforms.Resize((image_size, image_size)),\n",
    "                    transforms.RandomRotation(20),\n",
    "                    transforms.RandomAffine(\n",
    "                        degrees=15, \n",
    "                        translate=(0.15, 0.15), \n",
    "                        scale=(0.85, 1.15)\n",
    "                    ),\n",
    "                    transforms.ColorJitter(\n",
    "                        brightness=0.2,\n",
    "                        contrast=0.2,\n",
    "                        saturation=0.1\n",
    "                    ),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomVerticalFlip(p=0.2),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=CONFIG['normalize_means'],\n",
    "                        std=CONFIG['normalize_stds']\n",
    "                    ),\n",
    "                    transforms.RandomErasing(p=0.3, scale=(0.02, 0.15)),\n",
    "                ])\n",
    "            else:  # moderate or default\n",
    "                return transforms.Compose([\n",
    "                    transforms.Resize((image_size, image_size)),\n",
    "                    transforms.RandomHorizontalFlip(p=0.5),\n",
    "                    transforms.RandomRotation(10),\n",
    "                    transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "                    transforms.ToTensor(),\n",
    "                    transforms.Normalize(\n",
    "                        mean=CONFIG['normalize_means'],\n",
    "                        std=CONFIG['normalize_stds']\n",
    "                    ),\n",
    "                ])\n",
    "        else:  # test/val transforms\n",
    "            return transforms.Compose([\n",
    "                transforms.Resize((image_size, image_size)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(\n",
    "                    mean=CONFIG['normalize_means'],\n",
    "                    std=CONFIG['normalize_stds']\n",
    "                ),\n",
    "            ])\n",
    "\n",
    "# CutMix implementation\n",
    "def cutmix(data, target, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0)).to(data.device)\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    lam = max(lam, 1 - lam)\n",
    "    \n",
    "    batch_size = data.size(0)\n",
    "    bbx1, bby1, bbx2, bby2 = rand_bbox(data.size(), lam)\n",
    "    data[:, :, bbx1:bbx2, bby1:bby2] = shuffled_data[:, :, bbx1:bbx2, bby1:bby2]\n",
    "    \n",
    "    # Adjust lambda to exactly match pixel ratio\n",
    "    lam = 1 - ((bbx2 - bbx1) * (bby2 - bby1) / (data.size(-1) * data.size(-2)))\n",
    "    \n",
    "    return data, target, shuffled_target, lam\n",
    "\n",
    "def rand_bbox(size, lam):\n",
    "    W = size[2]\n",
    "    H = size[3]\n",
    "    cut_rat = np.sqrt(1. - lam)\n",
    "    cut_w = int(W * cut_rat)\n",
    "    cut_h = int(H * cut_rat)\n",
    "\n",
    "    # Uniform\n",
    "    cx = np.random.randint(W)\n",
    "    cy = np.random.randint(H)\n",
    "\n",
    "    bbx1 = np.clip(cx - cut_w // 2, 0, W)\n",
    "    bby1 = np.clip(cy - cut_h // 2, 0, H)\n",
    "    bbx2 = np.clip(cx + cut_w // 2, 0, W)\n",
    "    bby2 = np.clip(cy + cut_h // 2, 0, H)\n",
    "\n",
    "    return bbx1, bby1, bbx2, bby2\n",
    "\n",
    "# MixUp implementation\n",
    "def mixup(data, target, alpha=1.0):\n",
    "    indices = torch.randperm(data.size(0)).to(data.device)\n",
    "    shuffled_data = data[indices]\n",
    "    shuffled_target = target[indices]\n",
    "\n",
    "    lam = np.random.beta(alpha, alpha)\n",
    "    lam = max(lam, 1 - lam)\n",
    "    \n",
    "    data = lam * data + (1 - lam) * shuffled_data\n",
    "    \n",
    "    return data, target, shuffled_target, lam\n",
    "\n",
    "# Multi-scale training\n",
    "def get_multiscale_sizes(base_size=384, scales=[0.75, 1.0, 1.25]):\n",
    "    return [int(base_size * scale) for scale in scales]\n",
    "\n",
    "# High-performance BEiT model with attention pooling\n",
    "class HighPerformanceBeitModel(nn.Module):\n",
    "    def __init__(self, num_classes=2, pretrained=True, dropout_rate=0.3):\n",
    "        super(HighPerformanceBeitModel, self).__init__()\n",
    "        \n",
    "        # Use timm for advanced model features\n",
    "        if CONFIG.get('use_timm', False):\n",
    "            self.backbone = timm.create_model(\n",
    "                CONFIG['timm_model'],\n",
    "                pretrained=pretrained,\n",
    "                num_classes=0,\n",
    "                drop_rate=dropout_rate,\n",
    "                drop_path_rate=CONFIG.get('stochastic_depth_prob', 0.1)\n",
    "            )\n",
    "            print(f\"Created timm model: {CONFIG['timm_model']}\")\n",
    "            feature_dim = self.backbone.num_features\n",
    "        else:\n",
    "            # Use transformers BEiT model\n",
    "            model_name = CONFIG['beit_model']\n",
    "            self.backbone = BeitModel.from_pretrained(model_name)\n",
    "            print(f\"Created transformers model: {model_name}\")\n",
    "            feature_dim = self.backbone.config.hidden_size\n",
    "            \n",
    "        # Layer normalization\n",
    "        self.norm = nn.LayerNorm(feature_dim)\n",
    "        \n",
    "        # Attention pooling\n",
    "        if CONFIG.get('use_attention_pooling', False):\n",
    "            self.attention_pool = nn.Sequential(\n",
    "                nn.Linear(feature_dim, feature_dim // 2),\n",
    "                nn.Tanh(),\n",
    "                nn.Linear(feature_dim // 2, 1)\n",
    "            )\n",
    "        \n",
    "        # Multiple classifier heads\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(feature_dim, 1024),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(1024),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.GELU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Dropout(dropout_rate),\n",
    "            nn.Linear(512, num_classes)\n",
    "        )\n",
    "        \n",
    "        # Deep supervision\n",
    "        if CONFIG.get('use_deep_supervision', False):\n",
    "            self.aux_classifier = nn.Linear(feature_dim, num_classes)\n",
    "        \n",
    "        # Apply weight initialization\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        # Initialize weights for better convergence\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                torch.nn.init.trunc_normal_(m.weight, std=0.02)\n",
    "                if m.bias is not None:\n",
    "                    torch.nn.init.zeros_(m.bias)\n",
    "            elif isinstance(m, nn.LayerNorm):\n",
    "                torch.nn.init.ones_(m.weight)\n",
    "                torch.nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def attention_pooling(self, embeddings):\n",
    "        # Apply attention mechanism\n",
    "        attention_weights = self.attention_pool(embeddings)\n",
    "        attention_weights = F.softmax(attention_weights, dim=1)\n",
    "        attended_embedding = torch.sum(attention_weights * embeddings, dim=1)\n",
    "        return attended_embedding\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if CONFIG.get('use_timm', False):\n",
    "            # TIMM backbone\n",
    "            features = self.backbone(x)\n",
    "            \n",
    "            # Apply normalization\n",
    "            normalized = self.norm(features)\n",
    "            \n",
    "            # Apply attention pooling if enabled\n",
    "            if CONFIG.get('use_attention_pooling', False):\n",
    "                pooled = self.attention_pooling(normalized)\n",
    "                logits = self.classifier(pooled)\n",
    "            else:\n",
    "                logits = self.classifier(normalized)\n",
    "            \n",
    "            # Apply deep supervision if enabled\n",
    "            if CONFIG.get('use_deep_supervision', False) and self.training:\n",
    "                aux_logits = self.aux_classifier(normalized)\n",
    "                return logits, aux_logits\n",
    "            else:\n",
    "                return logits\n",
    "        else:\n",
    "            # Transformers backbone\n",
    "            outputs = self.backbone(x)\n",
    "            \n",
    "            # Get CLS token\n",
    "            cls_token = outputs.last_hidden_state[:, 0]\n",
    "            \n",
    "            # Apply normalization\n",
    "            normalized = self.norm(cls_token)\n",
    "            \n",
    "            # Apply classifier\n",
    "            logits = self.classifier(normalized)\n",
    "            \n",
    "            # Apply deep supervision if enabled\n",
    "            if CONFIG.get('use_deep_supervision', False) and self.training:\n",
    "                aux_logits = self.aux_classifier(normalized)\n",
    "                return logits, aux_logits\n",
    "            else:\n",
    "                return logits\n",
    "\n",
    "# Combined loss function\n",
    "class CombinedLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.5, gamma=2.0, class_weights=None, label_smoothing=0.1):\n",
    "        super(CombinedLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.class_weights = class_weights\n",
    "        self.label_smoothing = label_smoothing\n",
    "        self.ce_loss = nn.CrossEntropyLoss(\n",
    "            weight=class_weights,\n",
    "            label_smoothing=label_smoothing,\n",
    "            reduction='none'\n",
    "        )\n",
    "    \n",
    "    def forward(self, logits, targets, aux_logits=None, aux_targets=None, lam=None, targets_b=None):\n",
    "        # Calculate standard cross-entropy loss\n",
    "        ce = self.ce_loss(logits, targets)\n",
    "        \n",
    "        # Apply focal modulation if enabled\n",
    "        if self.gamma > 0:\n",
    "            pt = torch.exp(-ce)\n",
    "            focal_weight = (1 - pt) ** self.gamma\n",
    "            ce = focal_weight * ce\n",
    "        \n",
    "        # Apply auxiliary loss if provided\n",
    "        if aux_logits is not None and aux_targets is not None:\n",
    "            aux_ce = self.ce_loss(aux_logits, aux_targets)\n",
    "            if self.gamma > 0:\n",
    "                aux_pt = torch.exp(-aux_ce)\n",
    "                aux_focal_weight = (1 - aux_pt) ** self.gamma\n",
    "                aux_ce = aux_focal_weight * aux_ce\n",
    "            loss = ce.mean() * 0.8 + aux_ce.mean() * 0.2\n",
    "        else:\n",
    "            loss = ce.mean()\n",
    "        \n",
    "        # Apply mixup loss if provided\n",
    "        if lam is not None and targets_b is not None:\n",
    "            ce_b = self.ce_loss(logits, targets_b)\n",
    "            if self.gamma > 0:\n",
    "                pt_b = torch.exp(-ce_b)\n",
    "                focal_weight_b = (1 - pt_b) ** self.gamma\n",
    "                ce_b = focal_weight_b * ce_b\n",
    "            \n",
    "            # Apply mixup weighting\n",
    "            loss = lam * ce.mean() + (1 - lam) * ce_b.mean()\n",
    "            \n",
    "            # Apply auxiliary loss if provided\n",
    "            if aux_logits is not None and aux_targets is not None:\n",
    "                aux_ce_b = self.ce_loss(aux_logits, targets_b)\n",
    "                if self.gamma > 0:\n",
    "                    aux_pt_b = torch.exp(-aux_ce_b)\n",
    "                    aux_focal_weight_b = (1 - aux_pt_b) ** self.gamma\n",
    "                    aux_ce_b = aux_focal_weight_b * aux_ce_b\n",
    "                \n",
    "                # Combine main and auxiliary losses\n",
    "                aux_loss = lam * aux_ce.mean() + (1 - lam) * aux_ce_b.mean()\n",
    "                loss = loss * 0.8 + aux_loss * 0.2\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Test-time augmentation\n",
    "def test_time_augmentation(model, image, n_augments=5):\n",
    "    model.eval()\n",
    "    \n",
    "    # Base prediction\n",
    "    with torch.no_grad():\n",
    "        base_pred = model(image.unsqueeze(0))\n",
    "        base_pred = base_pred.softmax(dim=1) if isinstance(base_pred, torch.Tensor) else base_pred[0].softmax(dim=1)\n",
    "        \n",
    "    if n_augments <= 1:\n",
    "        return base_pred\n",
    "    \n",
    "    # Define TTA transforms\n",
    "    tta_transforms = [\n",
    "        transforms.RandomHorizontalFlip(p=1.0),\n",
    "        transforms.RandomVerticalFlip(p=1.0),\n",
    "        transforms.RandomRotation(degrees=10),\n",
    "        transforms.ColorJitter(brightness=0.1, contrast=0.1),\n",
    "        transforms.GaussianBlur(kernel_size=5, sigma=0.5),\n",
    "    ]\n",
    "    \n",
    "    # Original image dimensions\n",
    "    c, h, w = image.shape\n",
    "    \n",
    "    # Initialize list to store all predictions\n",
    "    all_preds = [base_pred]\n",
    "    \n",
    "    # Apply each transformation\n",
    "    with torch.no_grad():\n",
    "        for i in range(min(n_augments - 1, len(tta_transforms))):\n",
    "            # Apply transform\n",
    "            aug_image = tta_transforms[i](image)\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = model(aug_image.unsqueeze(0))\n",
    "            pred = pred.softmax(dim=1) if isinstance(pred, torch.Tensor) else pred[0].softmax(dim=1)\n",
    "            \n",
    "            # Add to predictions\n",
    "            all_preds.append(pred)\n",
    "    \n",
    "    # Average predictions\n",
    "    final_pred = torch.stack(all_preds).mean(dim=0)\n",
    "    \n",
    "    return final_pred\n",
    "\n",
    "# Continue the train_model function\n",
    "def train_model(model, dataloaders, dataset_sizes, criterion, optimizer, scheduler=None, num_epochs=30):\n",
    "    \"\"\"Enhanced training function with advanced techniques\"\"\"\n",
    "    since = time.time()\n",
    "    \n",
    "    # Initialize EMA if enabled\n",
    "    if CONFIG.get('exponential_moving_average', False):\n",
    "        ema_model = ModelEMA(model, decay=CONFIG.get('ema_decay', 0.999))\n",
    "    \n",
    "    # Set up checkpoint directory\n",
    "    checkpoint_dir = 'high_acc_checkpoints'\n",
    "    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "    \n",
    "    # Best model tracking\n",
    "    best_model_path = os.path.join(checkpoint_dir, 'best_model.pt')\n",
    "    best_acc = 0.0\n",
    "    best_f1 = 0.0\n",
    "    \n",
    "    # History tracking\n",
    "    history = {\n",
    "        'train_loss': [], 'train_acc': [],\n",
    "        'val_loss': [], 'val_acc': [],\n",
    "        'val_precision': [], 'val_recall': [], 'val_f1': [],\n",
    "        'val_specificity': [], 'val_sensitivity': [], 'lr': []\n",
    "    }\n",
    "    \n",
    "    # Gradient scaling for mixed precision\n",
    "    scaler = GradScaler(enabled=CONFIG.get('use_mixed_precision', True))\n",
    "    \n",
    "    # Early stopping\n",
    "    patience = CONFIG.get('patience', 10)\n",
    "    counter = 0\n",
    "    \n",
    "    # Progressive resizing\n",
    "    if CONFIG.get('use_progressive_resizing', False):\n",
    "        progressive_sizes = CONFIG.get('progressive_sizes', [256, 320, 384])\n",
    "        progressive_epochs = CONFIG.get('progressive_epochs', [10, 20, 30])\n",
    "        current_size_idx = 0\n",
    "    \n",
    "    # Learning rate warmup\n",
    "    warmup_epochs = CONFIG.get('warmup_epochs', 5)\n",
    "    \n",
    "    # Gradient accumulation steps\n",
    "    grad_accum_steps = CONFIG.get('gradient_accumulation_steps', 1)\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f'Epoch {epoch}/{num_epochs - 1}')\n",
    "        print('-' * 30)\n",
    "        \n",
    "        # Update image size for progressive resizing\n",
    "        if CONFIG.get('use_progressive_resizing', False):\n",
    "            if current_size_idx < len(progressive_epochs) and epoch >= progressive_epochs[current_size_idx]:\n",
    "                current_size = progressive_sizes[current_size_idx]\n",
    "                print(f\"Increasing image size to {current_size}\")\n",
    "                \n",
    "                # Update data transforms\n",
    "                for phase in ['train', 'val']:\n",
    "                    if phase in dataloaders:\n",
    "                        dataloaders[phase].dataset.transform = AdvancedTransforms.get_transforms(\n",
    "                            mode=phase,\n",
    "                            augmentation_strength=CONFIG.get('augmentation_strength', 'extreme'),\n",
    "                            image_size=current_size\n",
    "                        )\n",
    "                \n",
    "                current_size_idx = min(current_size_idx + 1, len(progressive_sizes) - 1)\n",
    "        \n",
    "        # Learning rate warmup\n",
    "        if epoch < warmup_epochs:\n",
    "            # Linear warmup\n",
    "            warmup_factor = (epoch + 1) / warmup_epochs\n",
    "            for param_group in optimizer.param_groups:\n",
    "                param_group['lr'] = param_group['initial_lr'] * warmup_factor\n",
    "        \n",
    "        # Store current learning rate\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        history['lr'].append(current_lr)\n",
    "        print(f\"Current learning rate: {current_lr:.7f}\")\n",
    "        \n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()\n",
    "            else:\n",
    "                model.eval()\n",
    "            \n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "            \n",
    "            # Progress bar for batches\n",
    "            pbar = tqdm(dataloaders[phase])\n",
    "            \n",
    "            # Reset gradient accumulation counter\n",
    "            accum_step = 0\n",
    "            \n",
    "            # Track all predictions for metrics calculation\n",
    "            all_labels = []\n",
    "            all_preds = []\n",
    "            all_probs = []\n",
    "            \n",
    "            # Process each batch\n",
    "            for inputs, labels in pbar:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "                \n",
    "                # Store labels for metrics\n",
    "                all_labels.append(labels.cpu().numpy())\n",
    "                \n",
    "                # Zero gradients for optimizer\n",
    "                if phase == 'train' and accum_step % grad_accum_steps == 0:\n",
    "                    optimizer.zero_grad()\n",
    "                \n",
    "                # Forward pass with mixed precision\n",
    "                with autocast(enabled=CONFIG.get('use_mixed_precision', True)):\n",
    "                    # Apply mixup or cutmix for training\n",
    "                    mixup_applied = False\n",
    "                    cutmix_applied = False\n",
    "                    lam = None\n",
    "                    labels_b = None\n",
    "                    \n",
    "                    if phase == 'train':\n",
    "                        # Apply mixup with probability\n",
    "                        if np.random.rand() < CONFIG.get('mixup_prob', 0.5):\n",
    "                            inputs, labels_a, labels_b, lam = mixup(\n",
    "                                inputs, labels, alpha=CONFIG.get('mixup_alpha', 0.4)\n",
    "                            )\n",
    "                            mixup_applied = True\n",
    "                        # Apply cutmix with probability\n",
    "                        elif np.random.rand() < CONFIG.get('cutmix_prob', 0.5):\n",
    "                            inputs, labels_a, labels_b, lam = cutmix(\n",
    "                                inputs, labels, alpha=CONFIG.get('mixup_alpha', 0.4)\n",
    "                            )\n",
    "                            cutmix_applied = True\n",
    "                    \n",
    "                    # Forward pass through model\n",
    "                    with torch.set_grad_enabled(phase == 'train'):\n",
    "                        if CONFIG.get('use_deep_supervision', False) and phase == 'train':\n",
    "                            # Forward pass with deep supervision\n",
    "                            outputs, aux_outputs = model(inputs)\n",
    "                            if mixup_applied or cutmix_applied:\n",
    "                                loss = criterion(outputs, labels, aux_outputs, labels, lam, labels_b)\n",
    "                            else:\n",
    "                                loss = criterion(outputs, labels, aux_outputs, labels)\n",
    "                        else:\n",
    "                            # Standard forward pass\n",
    "                            outputs = model(inputs)\n",
    "                            if mixup_applied or cutmix_applied:\n",
    "                                loss = criterion(outputs, labels, lam=lam, targets_b=labels_b)\n",
    "                            else:\n",
    "                                loss = criterion(outputs, labels)\n",
    "                        \n",
    "                        # Scale loss for gradient accumulation\n",
    "                        if phase == 'train':\n",
    "                            loss = loss / grad_accum_steps\n",
    "                        \n",
    "                        # Get predictions\n",
    "                        _, preds = torch.max(outputs, 1)\n",
    "                        \n",
    "                        # Store predictions for metrics\n",
    "                        if phase == 'val':\n",
    "                            all_preds.append(preds.cpu().numpy())\n",
    "                            probs = F.softmax(outputs, dim=1)\n",
    "                            all_probs.append(probs.cpu().numpy())\n",
    "                        \n",
    "                        # Backward pass and optimize\n",
    "                        if phase == 'train':\n",
    "                            # Use gradient scaling\n",
    "                            scaler.scale(loss).backward()\n",
    "                            \n",
    "                            # Update weights if accumulated enough gradients\n",
    "                            if (accum_step + 1) % grad_accum_steps == 0:\n",
    "                                # Unscale for gradient clipping\n",
    "                                scaler.unscale_(optimizer)\n",
    "                                \n",
    "                                # Gradient clipping\n",
    "                                torch.nn.utils.clip_grad_norm_(\n",
    "                                    model.parameters(), \n",
    "                                    max_norm=CONFIG.get('clip_grad_norm', 1.0)\n",
    "                                )\n",
    "                                \n",
    "                                # Optimizer step\n",
    "                                scaler.step(optimizer)\n",
    "                                scaler.update()\n",
    "                                \n",
    "                                # Update EMA model if enabled\n",
    "                                if CONFIG.get('exponential_moving_average', False):\n",
    "                                    ema_model.update()\n",
    "                                \n",
    "                                # Step scheduler if provided and not in warmup\n",
    "                                if scheduler is not None and epoch >= warmup_epochs:\n",
    "                                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                                        # This will be stepped after the epoch\n",
    "                                        pass\n",
    "                                    else:\n",
    "                                        scheduler.step()\n",
    "                            \n",
    "                            # Increment accumulation step\n",
    "                            accum_step += 1\n",
    "                \n",
    "                # Calculate statistics\n",
    "                if not (mixup_applied or cutmix_applied):\n",
    "                    batch_loss = loss.item() * inputs.size(0)\n",
    "                    batch_corrects = torch.sum(preds == labels.data)\n",
    "                    running_loss += batch_loss\n",
    "                    running_corrects += batch_corrects\n",
    "                    \n",
    "                    # Update progress bar\n",
    "                    current_loss_avg = running_loss / ((pbar.n + 1) * inputs.size(0))\n",
    "                    current_acc = running_corrects.double() / ((pbar.n + 1) * inputs.size(0))\n",
    "                    pbar.set_postfix({\n",
    "                        'loss': f'{current_loss_avg:.4f}',\n",
    "                        'acc': f'{current_acc:.4f}'\n",
    "                    })\n",
    "                \n",
    "                # Clean up memory\n",
    "                del inputs, outputs, loss\n",
    "                if phase == 'train' and (mixup_applied or cutmix_applied):\n",
    "                    del labels_b\n",
    "                \n",
    "                # Force garbage collection\n",
    "                torch.cuda.empty_cache()\n",
    "                gc.collect()\n",
    "            \n",
    "            # Calculate epoch metrics\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "            \n",
    "            # Record metrics\n",
    "            if phase == 'train':\n",
    "                history['train_loss'].append(epoch_loss)\n",
    "                history['train_acc'].append(epoch_acc.item())\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "            else:  # validation phase\n",
    "                # Process all predictions for metrics\n",
    "                all_labels = np.concatenate(all_labels)\n",
    "                all_preds = np.concatenate(all_preds)\n",
    "                all_probs = np.concatenate(all_probs)\n",
    "                \n",
    "                # Calculate detailed metrics\n",
    "                precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "                recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "                f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "                \n",
    "                # Calculate confusion matrix\n",
    "                cm = confusion_matrix(all_labels, all_preds)\n",
    "                if cm.shape == (2, 2):\n",
    "                    tn, fp, fn, tp = cm.ravel()\n",
    "                    specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "                    sensitivity = recall  # Same as recall\n",
    "                else:\n",
    "                    specificity = 0\n",
    "                    sensitivity = 0\n",
    "                \n",
    "                # Record validation metrics\n",
    "                history['val_loss'].append(epoch_loss)\n",
    "                history['val_acc'].append(epoch_acc.item())\n",
    "                history['val_precision'].append(precision)\n",
    "                history['val_recall'].append(recall)\n",
    "                history['val_f1'].append(f1)\n",
    "                history['val_specificity'].append(specificity)\n",
    "                history['val_sensitivity'].append(sensitivity)\n",
    "                \n",
    "                # Print validation metrics\n",
    "                print(f'{phase} Loss: {epoch_loss:.4f} Acc: {epoch_acc:.4f}')\n",
    "                print(f'Precision: {precision:.4f} Recall: {recall:.4f} F1: {f1:.4f}')\n",
    "                print(f'Specificity: {specificity:.4f}')\n",
    "                \n",
    "                if cm.shape == (2, 2):\n",
    "                    print(f'Confusion Matrix: TN={tn}, FP={fp}, FN={fn}, TP={tp}')\n",
    "                \n",
    "                # Step ReduceLROnPlateau scheduler if used\n",
    "                if scheduler is not None and epoch >= warmup_epochs:\n",
    "                    if isinstance(scheduler, torch.optim.lr_scheduler.ReduceLROnPlateau):\n",
    "                        # Using F1 score as metric for scheduler\n",
    "                        scheduler.step(f1)\n",
    "                \n",
    "                # Check if model improved\n",
    "                improved = False\n",
    "                if epoch_acc > best_acc:\n",
    "                    best_acc = epoch_acc\n",
    "                    improved = True\n",
    "                    metric_name = 'accuracy'\n",
    "                if f1 > best_f1:\n",
    "                    best_f1 = f1\n",
    "                    improved = True\n",
    "                    metric_name = 'F1'\n",
    "                \n",
    "                # Save checkpoint if improved\n",
    "                if improved:\n",
    "                    counter = 0\n",
    "                    print(f\"Saving model (improved {metric_name})\")\n",
    "                    \n",
    "                    # Save model state\n",
    "                    if CONFIG.get('exponential_moving_average', False):\n",
    "                        # Apply EMA weights for saving\n",
    "                        ema_model.apply_shadow()\n",
    "                        model_state = model.state_dict()\n",
    "                        # Restore original weights\n",
    "                        ema_model.restore()\n",
    "                    else:\n",
    "                        model_state = model.state_dict()\n",
    "                    \n",
    "                    # Save checkpoint\n",
    "                    torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': model_state,\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'scheduler_state_dict': scheduler.state_dict() if scheduler else None,\n",
    "                        'best_acc': best_acc,\n",
    "                        'best_f1': best_f1,\n",
    "                        'history': history,\n",
    "                        'config': CONFIG,\n",
    "                    }, best_model_path)\n",
    "                    \n",
    "                    # Save epoch-specific model for ensemble\n",
    "                    if CONFIG.get('use_ensemble', False):\n",
    "                        torch.save({\n",
    "                            'epoch': epoch,\n",
    "                            'model_state_dict': model_state,\n",
    "                            'acc': epoch_acc.item(),\n",
    "                            'f1': f1\n",
    "                        }, os.path.join(checkpoint_dir, f'model_epoch_{epoch}.pt'))\n",
    "                else:\n",
    "                    # Increment early stopping counter\n",
    "                    counter += 1\n",
    "                    print(f\"Early stopping counter: {counter}/{patience}\")\n",
    "                    \n",
    "                    # Check if we reached target accuracy\n",
    "                    if epoch_acc >= CONFIG.get('target_accuracy', 0.99):\n",
    "                        print(f\"Reached target accuracy of {CONFIG.get('target_accuracy', 0.99):.4f}!\")\n",
    "                        break\n",
    "                \n",
    "                # Early stopping\n",
    "                if counter >= patience:\n",
    "                    print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
    "                    break\n",
    "        \n",
    "        print()\n",
    "    \n",
    "    time_elapsed = time.time() - since\n",
    "    print(f'Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s')\n",
    "    print(f'Best val Acc: {best_acc:.4f}, Best F1: {best_f1:.4f}')\n",
    "    \n",
    "    # Load best model weights\n",
    "    checkpoint = torch.load(best_model_path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    return model, history\n",
    "\n",
    "def create_dataloaders(data_dir, image_size=384, val_split=0.1):\n",
    "    \"\"\"Create dataloaders with validation split and advanced augmentations\"\"\"\n",
    "    # Get transformations\n",
    "    train_transform = AdvancedTransforms.get_transforms(\n",
    "        mode='train',\n",
    "        augmentation_strength=CONFIG.get('augmentation_strength', 'extreme'),\n",
    "        image_size=CONFIG.get('start_size', image_size)\n",
    "    )\n",
    "    \n",
    "    val_transform = AdvancedTransforms.get_transforms(\n",
    "        mode='val',\n",
    "        augmentation_strength='none',\n",
    "        image_size=CONFIG.get('start_size', image_size)\n",
    "    )\n",
    "    \n",
    "    test_transform = AdvancedTransforms.get_transforms(\n",
    "        mode='val',\n",
    "        augmentation_strength='none',\n",
    "        image_size=image_size\n",
    "    )\n",
    "    \n",
    "    # Load datasets\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'train'),\n",
    "        transform=train_transform\n",
    "    )\n",
    "    \n",
    "    # Create or load test dataset\n",
    "    test_dataset = datasets.ImageFolder(\n",
    "        os.path.join(data_dir, 'test'),\n",
    "        transform=test_transform\n",
    "    )\n",
    "    \n",
    "    # Create validation set from training data\n",
    "    if val_split > 0:\n",
    "        # Get train dataset size\n",
    "        train_size = len(train_dataset)\n",
    "        \n",
    "        # Calculate split sizes\n",
    "        val_size = int(train_size * val_split)\n",
    "        new_train_size = train_size - val_size\n",
    "        \n",
    "        # Split train dataset\n",
    "        train_subset, val_subset = torch.utils.data.random_split(\n",
    "            train_dataset, \n",
    "            [new_train_size, val_size]\n",
    "        )\n",
    "        \n",
    "        # Apply correct transforms to validation subset\n",
    "        val_subset.dataset.transform = val_transform\n",
    "    else:\n",
    "        train_subset = train_dataset\n",
    "        val_subset = None\n",
    "    \n",
    "    # Create class weights for balanced sampling\n",
    "    if CONFIG.get('balanced_sampler', False):\n",
    "        # Count classes in train set\n",
    "        targets = [label for _, label in train_dataset.samples]\n",
    "        class_counts = np.bincount(targets)\n",
    "        \n",
    "        # Calculate weights\n",
    "        class_weights = 1.0 / class_counts\n",
    "        weights = class_weights[targets]\n",
    "        \n",
    "        # Create sampler\n",
    "        train_sampler = torch.utils.data.WeightedRandomSampler(\n",
    "            weights=weights,\n",
    "            num_samples=len(train_subset),\n",
    "            replacement=True\n",
    "        )\n",
    "    else:\n",
    "        train_sampler = None\n",
    "    \n",
    "    # Create dataloaders\n",
    "    dataloaders = {\n",
    "        'train': torch.utils.data.DataLoader(\n",
    "            train_subset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            sampler=train_sampler,\n",
    "            shuffle=train_sampler is None,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        ),\n",
    "        'test': torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    }\n",
    "    \n",
    "    # Add validation dataloader if we have a validation set\n",
    "    if val_subset is not None:\n",
    "        dataloaders['val'] = torch.utils.data.DataLoader(\n",
    "            val_subset,\n",
    "            batch_size=CONFIG['batch_size'],\n",
    "            shuffle=False,\n",
    "            num_workers=0,\n",
    "            pin_memory=False\n",
    "        )\n",
    "    else:\n",
    "        # Use test set as validation\n",
    "        dataloaders['val'] = dataloaders['test']\n",
    "    \n",
    "    # Calculate dataset sizes\n",
    "    dataset_sizes = {\n",
    "        'train': len(train_subset),\n",
    "        'val': len(val_subset) if val_subset is not None else len(test_dataset),\n",
    "        'test': len(test_dataset)\n",
    "    }\n",
    "    \n",
    "    # Get class names\n",
    "    class_names = train_dataset.classes\n",
    "    \n",
    "    return dataloaders, dataset_sizes, class_names\n",
    "\n",
    "def evaluate_model(model, dataloader, class_names):\n",
    "    \"\"\"Comprehensive model evaluation with test-time augmentation\"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    all_probs = []\n",
    "    \n",
    "    # Use test-time augmentation if enabled\n",
    "    use_tta = CONFIG.get('use_test_time_augmentation', False)\n",
    "    tta_transforms = CONFIG.get('tta_transforms', 5)\n",
    "    \n",
    "    # Process all batches\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(dataloader, desc=\"Evaluating\"):\n",
    "            all_labels.extend(labels.numpy())\n",
    "            \n",
    "            if use_tta:\n",
    "                # Process each image with TTA\n",
    "                batch_probs = []\n",
    "                for i in range(inputs.size(0)):\n",
    "                    # Apply TTA to single image\n",
    "                    img = inputs[i].to(device)\n",
    "                    probs = test_time_augmentation(model, img, n_augments=tta_transforms)\n",
    "                    batch_probs.append(probs.cpu().numpy())\n",
    "                \n",
    "                # Convert to array\n",
    "                batch_probs = np.array(batch_probs)\n",
    "                all_probs.extend(batch_probs)\n",
    "                \n",
    "                # Get predictions from probabilities\n",
    "                batch_preds = np.argmax(batch_probs, axis=1)\n",
    "                all_preds.extend(batch_preds)\n",
    "            else:\n",
    "                # Standard forward pass\n",
    "                inputs = inputs.to(device)\n",
    "                outputs = model(inputs)\n",
    "                _, preds = torch.max(outputs, 1)\n",
    "                \n",
    "                probs = F.softmax(outputs, dim=1)\n",
    "                \n",
    "                all_preds.extend(preds.cpu().numpy())\n",
    "                all_probs.extend(probs.cpu().numpy())\n",
    "            \n",
    "            # Clean up memory\n",
    "            del inputs\n",
    "            torch.cuda.empty_cache()\n",
    "    \n",
    "    # Convert to arrays\n",
    "    all_labels = np.array(all_labels)\n",
    "    all_preds = np.array(all_preds)\n",
    "    all_probs = np.array(all_probs)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    precision = precision_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    recall = recall_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    f1 = f1_score(all_labels, all_preds, average='binary', zero_division=0)\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    # Extract metrics from confusion matrix\n",
    "    if cm.shape == (2, 2):\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "        specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "        sensitivity = recall  # Same as recall\n",
    "    else:\n",
    "        specificity = 0\n",
    "        sensitivity = 0\n",
    "        tn, fp, fn, tp = 0, 0, 0, 0\n",
    "    \n",
    "    # Calculate AUC\n",
    "    try:\n",
    "        auc_score = roc_auc_score(all_labels, all_probs[:, 1])\n",
    "    except:\n",
    "        auc_score = 0\n",
    "    \n",
    "    # Print metrics\n",
    "    print(\"\\n===== FINAL MODEL EVALUATION =====\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"Precision: {precision:.4f}\")\n",
    "    print(f\"Recall (Sensitivity): {recall:.4f}\")\n",
    "    print(f\"Specificity: {specificity:.4f}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"AUC: {auc_score:.4f}\")\n",
    "    \n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(f\"  True Negatives: {tn}, False Positives: {fp}\")\n",
    "    print(f\"  False Negatives: {fn}, True Positives: {tp}\")\n",
    "    \n",
    "    # Plot confusion matrix\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=plt.cm.Blues)\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.colorbar()\n",
    "    plt.xticks([0, 1], class_names)\n",
    "    plt.yticks([0, 1], class_names)\n",
    "    \n",
    "    # Add text to cells\n",
    "    thresh = cm.max() / 2.\n",
    "    for i in range(cm.shape[0]):\n",
    "        for j in range(cm.shape[1]):\n",
    "            plt.text(j, i, format(cm[i, j], 'd'),\n",
    "                   horizontalalignment=\"center\", fontsize=16,\n",
    "                   color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    \n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('final_confusion_matrix.png', dpi=300)\n",
    "    \n",
    "    # Return all metrics\n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'specificity': specificity,\n",
    "        'f1': f1,\n",
    "        'auc': auc_score,\n",
    "        'confusion_matrix': cm,\n",
    "        'true_negatives': tn,\n",
    "        'false_positives': fp,\n",
    "        'false_negatives': fn,\n",
    "        'true_positives': tp\n",
    "    }\n",
    "\n",
    "def create_model_ensemble(checkpoint_dir, class_names, top_k=3, weights=None):\n",
    "    \"\"\"Create ensemble from multiple model checkpoints\"\"\"\n",
    "    # Find all model checkpoints\n",
    "    checkpoint_files = [f for f in os.listdir(checkpoint_dir) if f.startswith('model_epoch_')]\n",
    "    \n",
    "    if len(checkpoint_files) == 0:\n",
    "        print(\"No checkpoint files found for ensemble\")\n",
    "        return None\n",
    "    \n",
    "    # Load metrics for each checkpoint\n",
    "    checkpoint_metrics = []\n",
    "    for file in checkpoint_files:\n",
    "        try:\n",
    "            checkpoint = torch.load(os.path.join(checkpoint_dir, file))\n",
    "            checkpoint_metrics.append({\n",
    "                'file': file,\n",
    "                'acc': checkpoint.get('acc', 0),\n",
    "                'f1': checkpoint.get('f1', 0),\n",
    "                'epoch': checkpoint.get('epoch', 0)\n",
    "            })\n",
    "        except:\n",
    "            print(f\"Could not load checkpoint {file}\")\n",
    "    \n",
    "    # Sort by F1 score\n",
    "    checkpoint_metrics = sorted(checkpoint_metrics, key=lambda x: x['f1'], reverse=True)\n",
    "    \n",
    "    # Take top K models\n",
    "    top_models = checkpoint_metrics[:top_k]\n",
    "    \n",
    "    print(f\"Using top {top_k} models for ensemble:\")\n",
    "    for i, model_info in enumerate(top_models):\n",
    "        print(f\"  {i+1}. {model_info['file']} - F1: {model_info['f1']:.4f}, Acc: {model_info['acc']:.4f}\")\n",
    "    \n",
    "    # Create models\n",
    "    models = []\n",
    "    for model_info in top_models:\n",
    "        # Create model\n",
    "        model = HighPerformanceBeitModel(num_classes=len(class_names))\n",
    "        \n",
    "        # Load weights\n",
    "        checkpoint = torch.load(os.path.join(checkpoint_dir, model_info['file']))\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        \n",
    "        # Set to evaluation mode\n",
    "        model.eval()\n",
    "        \n",
    "        # Add to list\n",
    "        models.append(model.to(device))\n",
    "    \n",
    "    # Create ensemble weights if not provided\n",
    "    if weights is None:\n",
    "        # Normalize F1 scores to get weights\n",
    "        total_f1 = sum(model_info['f1'] for model_info in top_models)\n",
    "        weights = [model_info['f1'] / total_f1 for model_info in top_models]\n",
    "    \n",
    "    # Ensure weights sum to 1\n",
    "    weights = np.array(weights) / sum(weights)\n",
    "    \n",
    "    print(f\"Ensemble weights: {weights}\")\n",
    "    \n",
    "    return models, weights\n",
    "\n",
    "def predict_with_ensemble(models, weights, inputs, use_tta=False, tta_transforms=5):\n",
    "    \"\"\"Make prediction using ensemble of models\"\"\"\n",
    "    all_probs = []\n",
    "    \n",
    "    for i, model in enumerate(models):\n",
    "        # Apply test-time augmentation if enabled\n",
    "        if use_tta:\n",
    "            batch_probs = []\n",
    "            for j in range(inputs.size(0)):\n",
    "                # Apply TTA to single image\n",
    "                img = inputs[j].to(device)\n",
    "                probs = test_time_augmentation(model, img, n_augments=tta_transforms)\n",
    "                batch_probs.append(probs.cpu().numpy())\n",
    "            \n",
    "            # Convert to array\n",
    "            batch_probs = np.array(batch_probs)\n",
    "        else:\n",
    "            # Standard forward pass\n",
    "            with torch.no_grad():\n",
    "                outputs = model(inputs)\n",
    "                probs = F.softmax(outputs, dim=1).cpu().numpy()\n",
    "                batch_probs = probs\n",
    "        \n",
    "        # Apply weight\n",
    "        weighted_probs = batch_probs * weights[i]\n",
    "        all_probs.append(weighted_probs)\n",
    "    \n",
    "    # Sum weighted probabilities\n",
    "    final_probs = sum(all_probs)\n",
    "    \n",
    "    # Get predictions from probabilities\n",
    "    final_preds = np.argmax(final_probs, axis=1)\n",
    "    \n",
    "    return final_preds, final_probs\n",
    "\n",
    "def plot_training_history(history):\n",
    "    \"\"\"Plot detailed training history\"\"\"\n",
    "    plt.figure(figsize=(20, 15))\n",
    "    \n",
    "    # Plot loss\n",
    "    plt.subplot(3, 2, 1)\n",
    "    plt.plot(history['train_loss'], label='Train Loss')\n",
    "    plt.plot(history['val_loss'], label='Validation Loss')\n",
    "    plt.title('Loss vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Loss', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot accuracy\n",
    "    plt.subplot(3, 2, 2)\n",
    "    plt.plot(history['train_acc'], label='Train Accuracy')\n",
    "    plt.plot(history['val_acc'], label='Validation Accuracy')\n",
    "    plt.title('Accuracy vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Accuracy', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot precision and recall\n",
    "    plt.subplot(3, 2, 3)\n",
    "    plt.plot(history['val_precision'], label='Precision')\n",
    "    plt.plot(history['val_recall'], label='Recall')\n",
    "    plt.title('Precision & Recall vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot F1 score\n",
    "    plt.subplot(3, 2, 4)\n",
    "    plt.plot(history['val_f1'], label='F1 Score')\n",
    "    plt.title('F1 Score vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('F1 Score', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot specificity and sensitivity\n",
    "    plt.subplot(3, 2, 5)\n",
    "    plt.plot(history['val_specificity'], label='Specificity')\n",
    "    plt.plot(history['val_sensitivity'], label='Sensitivity')\n",
    "    plt.title('Specificity & Sensitivity vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Score', fontsize=12)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Plot learning rate\n",
    "    plt.subplot(3, 2, 6)\n",
    "    plt.plot(history['lr'], label='Learning Rate')\n",
    "    plt.title('Learning Rate vs. Epoch', fontsize=14)\n",
    "    plt.xlabel('Epoch', fontsize=12)\n",
    "    plt.ylabel('Learning Rate', fontsize=12)\n",
    "    plt.yscale('log')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('training_history.png', dpi=300)\n",
    "    \n",
    "def main():\n",
    "    \"\"\"Main function to run the training and evaluation\"\"\"\n",
    "    print(\"=== High Accuracy BEiT Pneumonia Detection ===\")\n",
    "    \n",
    "    # Set seed for reproducibility\n",
    "    set_seed(42)\n",
    "    \n",
    "    # Create dataloaders\n",
    "    print(\"Creating dataloaders with advanced augmentations...\")\n",
    "    dataloaders, dataset_sizes, class_names = create_dataloaders(\n",
    "        data_dir=CONFIG['data_dir'],\n",
    "        image_size=CONFIG['image_size'],\n",
    "        val_split=CONFIG.get('val_percent', 0.1)\n",
    "    )\n",
    "    \n",
    "    print(f\"Classes: {class_names}\")\n",
    "    print(f\"Dataset sizes: {dataset_sizes}\")\n",
    "    \n",
    "    # Create model\n",
    "    print(\"Creating high-performance BEiT model...\")\n",
    "    model = HighPerformanceBeitModel(\n",
    "        num_classes=len(class_names),\n",
    "        pretrained=True,\n",
    "        dropout_rate=CONFIG.get('dropout_rate', 0.3)\n",
    "    )\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Count parameters\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Total parameters: {total_params:,}\")\n",
    "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "    \n",
    "    # Create loss function\n",
    "    if CONFIG.get('use_weighted_loss', True):\n",
    "        # Setup class weights\n",
    "        weight_normal = CONFIG.get('class_weight_normal', 1.5) \n",
    "        weight_pneumonia = CONFIG.get('class_weight_pneumonia', 1.0)\n",
    "        class_weights = torch.tensor([weight_normal, weight_pneumonia]).float().to(device)\n",
    "        \n",
    "        # Create combined loss\n",
    "        criterion = CombinedLoss(\n",
    "            alpha=0.5,\n",
    "            gamma=CONFIG.get('focal_loss_gamma', 2.0),\n",
    "            class_weights=class_weights,\n",
    "            label_smoothing=CONFIG.get('label_smoothing', 0.1)\n",
    "        )\n",
    "        print(f\"Using Combined Loss with class weights [{weight_normal}, {weight_pneumonia}]\")\n",
    "    else:\n",
    "        # Standard cross entropy\n",
    "        criterion = nn.CrossEntropyLoss(\n",
    "            label_smoothing=CONFIG.get('label_smoothing', 0.1)\n",
    "        )\n",
    "        print(\"Using CrossEntropyLoss with label smoothing\")\n",
    "    \n",
    "    # Create optimizer with discriminative learning rates\n",
    "    if CONFIG.get('discriminative_lr', False):\n",
    "        # Group parameters by layers\n",
    "        backbone_params = list(model.backbone.parameters())\n",
    "        classifier_params = list(model.classifier.parameters())\n",
    "        \n",
    "        # Add other parameters if they exist\n",
    "        other_params = []\n",
    "        if hasattr(model, 'norm'):\n",
    "            other_params.extend(list(model.norm.parameters()))\n",
    "        if hasattr(model, 'attention_pool'):\n",
    "            other_params.extend(list(model.attention_pool.parameters()))\n",
    "        if hasattr(model, 'aux_classifier'):\n",
    "            other_params.extend(list(model.aux_classifier.parameters()))\n",
    "        \n",
    "        # Create parameter groups with different learning rates\n",
    "        lr = CONFIG['learning_rate']\n",
    "        factor = CONFIG.get('discriminative_lr_factor', 0.1)\n",
    "        param_groups = [\n",
    "            {'params': backbone_params, 'lr': lr * factor, 'initial_lr': lr * factor},\n",
    "            {'params': other_params, 'lr': lr * (factor * 5), 'initial_lr': lr * (factor * 5)},\n",
    "            {'params': classifier_params, 'lr': lr, 'initial_lr': lr}\n",
    "        ]\n",
    "        print(f\"Using discriminative learning rates: {lr * factor}, {lr * (factor * 5)}, {lr}\")\n",
    "    else:\n",
    "        # Single learning rate for all parameters\n",
    "        param_groups = model.parameters()\n",
    "        lr = CONFIG['learning_rate']\n",
    "        print(f\"Using single learning rate: {lr}\")\n",
    "    \n",
    "    # Create optimizer\n",
    "    optimizer = optim.AdamW(\n",
    "        param_groups,\n",
    "        lr=CONFIG['learning_rate'],\n",
    "        weight_decay=CONFIG.get('weight_decay', 0.05),\n",
    "        eps=1e-8\n",
    "    )\n",
    "    \n",
    "    # Apply Lookahead if enabled\n",
    "    if CONFIG.get('lookahead_optimizer', False):\n",
    "        optimizer = Lookahead(\n",
    "            optimizer,\n",
    "            k=CONFIG.get('lookahead_k', 5),\n",
    "            alpha=CONFIG.get('lookahead_alpha', 0.5)\n",
    "        )\n",
    "        print(\"Using Lookahead optimizer wrapper\")\n",
    "    \n",
    "    # Create scheduler\n",
    "    if CONFIG.get('scheduler', 'cosine_with_warmup') == 'cosine_with_warmup':\n",
    "        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            optimizer,\n",
    "            T_max=CONFIG['num_epochs'],\n",
    "            eta_min=CONFIG.get('min_lr', 1e-7)\n",
    "        )\n",
    "        print(\"Using CosineAnnealingLR scheduler\")\n",
    "    else:\n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer,\n",
    "            mode='max',\n",
    "            factor=CONFIG.get('plateau_factor', 0.5),\n",
    "            patience=CONFIG.get('plateau_patience', 5),\n",
    "            verbose=True,\n",
    "            min_lr=CONFIG.get('min_lr', 1e-7)\n",
    "        )\n",
    "        print(\"Using ReduceLROnPlateau scheduler\")\n",
    "    \n",
    "    # Train model\n",
    "    print(f\"Starting training for {CONFIG['num_epochs']} epochs...\")\n",
    "    \n",
    "    # Check if a checkpoint should be loaded\n",
    "    start_from_checkpoint = False\n",
    "    best_model_path = os.path.join('high_acc_checkpoints', 'best_model.pt')\n",
    "    \n",
    "    if os.path.exists(best_model_path):\n",
    "        print(f\"Found existing checkpoint at {best_model_path}\")\n",
    "        response = input(\"Continue from checkpoint? (y/n): \")\n",
    "        if response.lower() == 'y':\n",
    "            try:\n",
    "                checkpoint = torch.load(best_model_path)\n",
    "                model.load_state_dict(checkpoint['model_state_dict'])\n",
    "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "                if scheduler is not None and 'scheduler_state_dict' in checkpoint and checkpoint['scheduler_state_dict']:\n",
    "                    scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
    "                    \n",
    "                start_epoch = checkpoint.get('epoch', 0) + 1\n",
    "                best_acc = checkpoint.get('best_acc', 0)\n",
    "                best_f1 = checkpoint.get('best_f1', 0)\n",
    "                history = checkpoint.get('history', {})\n",
    "                \n",
    "                print(f\"Loaded checkpoint from epoch {start_epoch-1}\")\n",
    "                print(f\"Best accuracy so far: {best_acc:.4f}, Best F1: {best_f1:.4f}\")\n",
    "                \n",
    "                start_from_checkpoint = True\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading checkpoint: {e}\")\n",
    "                print(\"Starting fresh training\")\n",
    "                start_from_checkpoint = False\n",
    "    \n",
    "    # Train the model\n",
    "    model, history = train_model(\n",
    "        model=model,\n",
    "        dataloaders=dataloaders,\n",
    "        dataset_sizes=dataset_sizes,\n",
    "        criterion=criterion,\n",
    "        optimizer=optimizer,\n",
    "        scheduler=scheduler,\n",
    "        num_epochs=CONFIG['num_epochs']\n",
    "    )\n",
    "    \n",
    "    # Plot training history\n",
    "    if CONFIG.get('training_visualization', True):\n",
    "        plot_training_history(history)\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    print(\"\\nEvaluating final model on test set...\")\n",
    "    metrics = evaluate_model(model, dataloaders['test'], class_names)\n",
    "    \n",
    "    # Create ensemble if enabled\n",
    "    if CONFIG.get('use_ensemble', True):\n",
    "        print(\"\\nCreating model ensemble...\")\n",
    "        ensemble_models, ensemble_weights = create_model_ensemble(\n",
    "            checkpoint_dir='high_acc_checkpoints',\n",
    "            class_names=class_names,\n",
    "            top_k=3,\n",
    "            weights=CONFIG.get('ensemble_weights', None)\n",
    "        )\n",
    "        \n",
    "        if ensemble_models:\n",
    "            print(\"Evaluating ensemble model...\")\n",
    "            \n",
    "            # Process test set with ensemble\n",
    "            ensemble_all_labels = []\n",
    "            ensemble_all_preds = []\n",
    "            \n",
    "            for inputs, labels in tqdm(dataloaders['test'], desc=\"Ensemble evaluation\"):\n",
    "                ensemble_all_labels.extend(labels.numpy())\n",
    "                \n",
    "                inputs = inputs.to(device)\n",
    "                preds, _ = predict_with_ensemble(\n",
    "                    ensemble_models, \n",
    "                    ensemble_weights, \n",
    "                    inputs,\n",
    "                    use_tta=CONFIG.get('use_test_time_augmentation', True),\n",
    "                    tta_transforms=CONFIG.get('tta_transforms', 5)\n",
    "                )\n",
    "                \n",
    "                ensemble_all_preds.extend(preds)\n",
    "                \n",
    "                # Clean up memory\n",
    "                del inputs\n",
    "                torch.cuda.empty_cache()\n",
    "            \n",
    "            # Calculate ensemble metrics\n",
    "            ensemble_accuracy = accuracy_score(ensemble_all_labels, ensemble_all_preds)\n",
    "            ensemble_precision = precision_score(ensemble_all_labels, ensemble_all_preds, average='binary', zero_division=0)\n",
    "            ensemble_recall = recall_score(ensemble_all_labels, ensemble_all_preds, average='binary', zero_division=0)\n",
    "            ensemble_f1 = f1_score(ensemble_all_labels, ensemble_all_preds, average='binary', zero_division=0)\n",
    "            \n",
    "            print(f\"\\nEnsemble Results:\")\n",
    "            print(f\"Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(f\"Precision: {ensemble_precision:.4f}\")\n",
    "            print(f\"Recall: {ensemble_recall:.4f}\")\n",
    "            print(f\"F1 Score: {ensemble_f1:.4f}\")\n",
    "            \n",
    "            # Compare with single model\n",
    "            print(\"\\nComparison:\")\n",
    "            print(f\"Single Model Accuracy: {metrics['accuracy']:.4f}\")\n",
    "            print(f\"Ensemble Model Accuracy: {ensemble_accuracy:.4f}\")\n",
    "            print(f\"Improvement: {(ensemble_accuracy - metrics['accuracy']) * 100:.2f}%\")\n",
    "    \n",
    "    # Create HTML report\n",
    "    report_html = f\"\"\"\n",
    "    <!DOCTYPE html>\n",
    "    <html lang=\"en\">\n",
    "    <head>\n",
    "        <meta charset=\"UTF-8\">\n",
    "        <meta name=\"viewport\" content=\"width=device-width, initial-scale=1.0\">\n",
    "        <title>Pneumonia Detection Model Report</title>\n",
    "        <style>\n",
    "            body {{ font-family: Arial, sans-serif; line-height: 1.6; margin: 0; padding: 20px; color: #333; }}\n",
    "            .container {{ max-width: 1200px; margin: 0 auto; }}\n",
    "            h1 {{ color: #2c3e50; border-bottom: 2px solid #3498db; padding-bottom: 10px; }}\n",
    "            h2 {{ color: #3498db; margin-top: 30px; }}\n",
    "            .metric-container {{ display: flex; flex-wrap: wrap; gap: 20px; margin: 20px 0; }}\n",
    "            .metric-box {{ background-color: #f8f9fa; border-radius: 8px; padding: 20px; box-shadow: 0 2px 10px rgba(0,0,0,0.1); width: 200px; }}\n",
    "            .metric-value {{ font-size: 28px; font-weight: bold; color: #2980b9; margin: 10px 0; }}\n",
    "            .metric-name {{ font-weight: bold; font-size: 16px; }}\n",
    "            .alert {{ background-color: #e74c3c; color: white; padding: 10px; border-radius: 5px; margin: 20px 0; }}\n",
    "            .success {{ background-color: #2ecc71; color: white; padding: 10px; border-radius: 5px; margin: 20px 0; }}\n",
    "            table {{ border-collapse: collapse; width: 100%; margin: 20px 0; }}\n",
    "            th, td {{ border: 1px solid #ddd; padding: 12px; text-align: left; }}\n",
    "            th {{ background-color: #f2f2f2; }}\n",
    "            tr:nth-child(even) {{ background-color: #f9f9f9; }}\n",
    "            tr:hover {{ background-color: #f5f5f5; }}\n",
    "            img {{ max-width: 100%; height: auto; margin: 20px 0; border: 1px solid #ddd; }}\n",
    "            .image-container {{ margin: 30px 0; }}\n",
    "            .footer {{ margin-top: 50px; border-top: 1px solid #ddd; padding-top: 20px; font-size: 12px; color: #777; }}\n",
    "        </style>\n",
    "    </head>\n",
    "    <body>\n",
    "        <div class=\"container\">\n",
    "            <h1>Pneumonia Detection Model Report</h1>\n",
    "            \n",
    "            <h2>Model Performance</h2>\n",
    "            <div class=\"metric-container\">\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">Accuracy</div>\n",
    "                    <div class=\"metric-value\">{metrics['accuracy']:.4f}</div>\n",
    "                </div>\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">Precision</div>\n",
    "                    <div class=\"metric-value\">{metrics['precision']:.4f}</div>\n",
    "                </div>\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">Recall (Sensitivity)</div>\n",
    "                    <div class=\"metric-value\">{metrics['recall']:.4f}</div>\n",
    "                </div>\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">Specificity</div>\n",
    "                    <div class=\"metric-value\">{metrics['specificity']:.4f}</div>\n",
    "                </div>\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">F1 Score</div>\n",
    "                    <div class=\"metric-value\">{metrics['f1']:.4f}</div>\n",
    "                </div>\n",
    "                <div class=\"metric-box\">\n",
    "                    <div class=\"metric-name\">AUC</div>\n",
    "                    <div class=\"metric-value\">{metrics['auc']:.4f}</div>\n",
    "                </div>\n",
    "            </div>\n",
    "            \n",
    "            {'<div class=\"success\">✓ Model achieved target accuracy of 99%!</div>' if metrics['accuracy'] >= 0.99 else '<div class=\"alert\">⚠️ Model did not reach target accuracy of 99%.</div>'}\n",
    "            \n",
    "            <h2>Confusion Matrix</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th></th>\n",
    "                    <th>Predicted Normal</th>\n",
    "                    <th>Predicted Pneumonia</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Actual Normal</th>\n",
    "                    <td>{metrics['true_negatives']}</td>\n",
    "                    <td>{metrics['false_positives']}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <th>Actual Pneumonia</th>\n",
    "                    <td>{metrics['false_negatives']}</td>\n",
    "                    <td>{metrics['true_positives']}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "            \n",
    "            <h2>Training History</h2>\n",
    "            <div class=\"image-container\">\n",
    "                <img src=\"training_history.png\" alt=\"Training History\" />\n",
    "            </div>\n",
    "            \n",
    "            <h2>Confusion Matrix Visualization</h2>\n",
    "            <div class=\"image-container\">\n",
    "                <img src=\"final_confusion_matrix.png\" alt=\"Confusion Matrix\" />\n",
    "            </div>\n",
    "            \n",
    "            <h2>Model Configuration</h2>\n",
    "            <table>\n",
    "                <tr>\n",
    "                    <th>Parameter</th>\n",
    "                    <th>Value</th>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Model</td>\n",
    "                    <td>{CONFIG.get('timm_model', CONFIG.get('beit_model', 'Unknown'))}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Image Size</td>\n",
    "                    <td>{CONFIG['image_size']}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Learning Rate</td>\n",
    "                    <td>{CONFIG['learning_rate']}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Batch Size</td>\n",
    "                    <td>{CONFIG['batch_size']} (Effective: {CONFIG['batch_size'] * CONFIG['gradient_accumulation_steps']})</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Augmentation Strength</td>\n",
    "                    <td>{CONFIG.get('augmentation_strength', 'extreme')}</td>\n",
    "                </tr>\n",
    "                <tr>\n",
    "                    <td>Class Weights</td>\n",
    "                    <td>Normal: {CONFIG.get('class_weight_normal', 1.5)}, Pneumonia: {CONFIG.get('class_weight_pneumonia', 1.0)}</td>\n",
    "                </tr>\n",
    "            </table>\n",
    "            \n",
    "            <div class=\"footer\">\n",
    "                <p>Report generated on {time.strftime('%Y-%m-%d %H:%M:%S')}</p>\n",
    "                <p>High-Accuracy BEiT Pneumonia Detection Model</p>\n",
    "            </div>\n",
    "        </div>\n",
    "    </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    # Save HTML report\n",
    "    with open('pneumonia_detection_report.html', 'w') as f:\n",
    "        f.write(report_html)\n",
    "    \n",
    "    print(\"Generated HTML report: pneumonia_detection_report.html\")\n",
    "    \n",
    "    # Save the model\n",
    "    final_model_path = 'high_accuracy_pneumonia_model.pth'\n",
    "    torch.save({\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'config': CONFIG,\n",
    "        'class_names': class_names,\n",
    "        'metrics': metrics,\n",
    "        'history': history\n",
    "    }, final_model_path)\n",
    "    \n",
    "    print(f\"Saved final model to {final_model_path}\")\n",
    "    \n",
    "    # Create usage example\n",
    "    usage_code = \"\"\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def load_high_accuracy_model(model_path):\n",
    "    # Load checkpoint\n",
    "    checkpoint = torch.load(model_path, map_location='cpu')\n",
    "    \n",
    "    # Extract configuration\n",
    "    config = checkpoint.get('config', {})\n",
    "    class_names = checkpoint.get('class_names', ['NORMAL', 'PNEUMONIA'])\n",
    "    \n",
    "    # Create model\n",
    "    model = create_model(len(class_names), config)\n",
    "    \n",
    "    # Load weights\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    model.eval()\n",
    "    \n",
    "    return model, checkpoint\n",
    "\n",
    "def create_model(num_classes, config):\n",
    "    # Check if timm is being used\n",
    "    if config.get('use_timm', False):\n",
    "        import timm\n",
    "        \n",
    "        # Create timm-based model\n",
    "        model = timm.create_model(\n",
    "            config.get('timm_model', 'beit_base_patch16_224'),\n",
    "            pretrained=False,\n",
    "            num_classes=0\n",
    "        )\n",
    "        \n",
    "        # Add custom head\n",
    "        feature_dim = model.num_features\n",
    "        dropout_rate = config.get('dropout_rate', 0.3)\n",
    "        \n",
    "        # Create classifier layers\n",
    "        class CustomModel(nn.Module):\n",
    "            def __init__(self):\n",
    "                super().__init__()\n",
    "                self.backbone = model\n",
    "                self.norm = nn.LayerNorm(feature_dim)\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(feature_dim, 1024),\n",
    "                    nn.GELU(),\n",
    "                    nn.LayerNorm(1024),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.GELU(),\n",
    "                    nn.LayerNorm(512),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(512, num_classes)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                x = self.backbone(x)\n",
    "                x = self.norm(x)\n",
    "                x = self.classifier(x)\n",
    "                return x\n",
    "        \n",
    "        return CustomModel()\n",
    "    else:\n",
    "        # Use transformers BEiT model\n",
    "        from transformers import BeitModel\n",
    "        \n",
    "        # Define model architecture\n",
    "        class BeitClassifier(nn.Module):\n",
    "            def __init__(self, num_classes, dropout_rate=0.3):\n",
    "                super().__init__()\n",
    "                \n",
    "                # Load BEiT model\n",
    "                self.beit = BeitModel.from_pretrained(config.get('beit_model', 'microsoft/beit-base-patch16-224'))\n",
    "                \n",
    "                # Get feature dimension\n",
    "                self.feature_dim = self.beit.config.hidden_size\n",
    "                \n",
    "                # Add layer norm\n",
    "                self.norm = nn.LayerNorm(self.feature_dim)\n",
    "                \n",
    "                # Add classifier\n",
    "                self.classifier = nn.Sequential(\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(self.feature_dim, 1024),\n",
    "                    nn.GELU(),\n",
    "                    nn.LayerNorm(1024),\n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(1024, 512),\n",
    "                    nn.GELU(),\n",
    "                    nn.LayerNorm(512), \n",
    "                    nn.Dropout(dropout_rate),\n",
    "                    nn.Linear(512, num_classes)\n",
    "                )\n",
    "            \n",
    "            def forward(self, x):\n",
    "                # Extract features\n",
    "                outputs = self.beit(x)\n",
    "                cls_token = outputs.last_hidden_state[:, 0]\n",
    "                \n",
    "                # Apply norm\n",
    "                normalized = self.norm(cls_token)\n",
    "                \n",
    "                # Apply classifier\n",
    "                logits = self.classifier(normalized)\n",
    "                \n",
    "                return logits\n",
    "        \n",
    "        return BeitClassifier(num_classes, dropout_rate=config.get('dropout_rate', 0.3))\n",
    "\n",
    "def predict_pneumonia(model, image_path, checkpoint=None):\n",
    "\n",
    "    # Get configuration\n",
    "    config = checkpoint.get('config', {}) if checkpoint else {}\n",
    "    class_names = checkpoint.get('class_names', ['NORMAL', 'PNEUMONIA']) if checkpoint else ['NORMAL', 'PNEUMONIA']\n",
    "    \n",
    "    # Get normalization parameters\n",
    "    normalize_means = config.get('normalize_means', [0.5056, 0.5056, 0.5056])\n",
    "    normalize_stds = config.get('normalize_stds', [0.252, 0.252, 0.252])\n",
    "    \n",
    "    # Create transform\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((config.get('image_size', 384), config.get('image_size', 384))),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=normalize_means, std=normalize_stds)\n",
    "    ])\n",
    "    \n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    input_tensor = transform(image).unsqueeze(0)\n",
    "    \n",
    "    # Get prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_tensor)\n",
    "        probabilities = torch.nn.functional.softmax(outputs, dim=1)[0]\n",
    "        predicted_class = torch.argmax(probabilities, dim=0).item()\n",
    "    \n",
    "    # Get class name and probability\n",
    "    class_name = class_names[predicted_class]\n",
    "    probability = probabilities[predicted_class].item() * 100\n",
    "    \n",
    "    # Get pneumonia probability\n",
    "    pneumonia_idx = class_names.index('PNEUMONIA') if 'PNEUMONIA' in class_names else 1\n",
    "    pneumonia_prob = probabilities[pneumonia_idx].item() * 100\n",
    "    \n",
    "    # Display results\n",
    "    print(f\"Prediction: {class_name}\")\n",
    "    print(f\"Confidence: {probability:.1f}%\")\n",
    "    print(f\"Pneumonia Probability: {pneumonia_prob:.1f}%\")\n",
    "    \n",
    "    # Display image with prediction\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.imshow(image)\n",
    "    plt.title(f\"Prediction: {class_name} ({probability:.1f}%)\", size=20)\n",
    "    plt.axis('off')\n",
    "    \n",
    "    # Add probability bars\n",
    "    plt.figure(figsize=(10, 3))\n",
    "    colors = ['green', 'red'] if pneumonia_idx == 1 else ['red', 'green']\n",
    "    bars = plt.bar(class_names, [probabilities[i].item() * 100 for i in range(len(class_names))], color=colors)\n",
    "    \n",
    "    # Add labels on bars\n",
    "    for bar in bars:\n",
    "        height = bar.get_height()\n",
    "        plt.text(\n",
    "            bar.get_x() + bar.get_width()/2.,\n",
    "            height + 1,\n",
    "            f'{height:.1f}%',\n",
    "            ha='center',\n",
    "            va='bottom',\n",
    "            fontsize=14\n",
    "        )\n",
    "    \n",
    "    plt.title('Prediction Probabilities', size=16)\n",
    "    plt.ylabel('Probability (%)', size=14)\n",
    "    plt.ylim(0, 100)\n",
    "    plt.xticks(fontsize=14)\n",
    "    plt.yticks(fontsize=12)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return {\n",
    "        'class': class_name,\n",
    "        'probability': probability,\n",
    "        'pneumonia_probability': pneumonia_prob\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Load the model\n",
    "    model, checkpoint = load_high_accuracy_model('high_accuracy_pneumonia_model.pth')\n",
    "    \n",
    "    # Make prediction\n",
    "    predict_pneumonia(model, 'path/to/your/xray.jpg', checkpoint)\n",
    "\"\"\"\n",
    "    \n",
    "    # Save usage example\n",
    "    with open('use_high_accuracy_model.py', 'w') as f:\n",
    "        f.write(usage_code)\n",
    "    \n",
    "    print(\"Saved usage example to 'use_high_accuracy_model.py'\")\n",
    "    \n",
    "    # Print final summary\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"HIGH ACCURACY PNEUMONIA DETECTION MODEL TRAINING COMPLETE\")\n",
    "    print(\"=\"*50)\n",
    "    print(f\"Final Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.4f}, Recall: {metrics['recall']:.4f}\")\n",
    "    print(f\"F1 Score: {metrics['f1']:.4f}, AUC: {metrics['auc']:.4f}\")\n",
    "    \n",
    "    if metrics['accuracy'] >= 0.99:\n",
    "        print(\"\\n✓ Successfully achieved target accuracy of 99%!\")\n",
    "    else:\n",
    "        print(f\"\\n Did not reach target accuracy of 99% (got {metrics['accuracy']:.2%}).\")\n",
    "        \n",
    "    print(\"\\nModel saved, report generated, and usage example created.\")\n",
    "    print(\"=\"*50)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
